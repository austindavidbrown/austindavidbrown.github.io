<html>
<head>
  <title></title>
  <meta name="author" content="Austin David Brown" />
  <meta name="date" content="2019-01-12" />
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
       displayMath: [['\\[', '\\]']],
       inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"></script>

  <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
  <style>
  body {
    word-wrap: break-word; /* stops words exceeding containers */
    text-size-adjust: 100% !important; /* Fixes mobile messing up text sizes */
  }
  algorithm {
    display: table;
    border: solid 1px;
    padding: 1em;
    margin: 1em;
    white-space: pre;
  }
  </style>
</head>
<body>

$
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
$

<h2>Estimating the Variance</h2>

This approach is to estimate the variance using the Markov chain.

<h3>Batch Means</h3>

Let $(X_k)$ be the Markov chain and $f : \mathbb{R}^d \to \mathbb{R}$.
For i.i.d. sampling, the concentration is controlled by $\frac{Var(f(X_1))}{n}$.
The functional CLT is the motivation here.
Divide the chain $(X_k)$ into $b$ non-overlapping batches so that $n = b m$.
Estimate the batch mean by $B_k = \frac{1}{b} \sum_{i = 1}^{b} f(X_{bk + i})$.
The estimate of the variance of the batch mean is

\[
S^2_{BM} = \frac{b}{n} \sum_{k = 0}^{m - 1} (B_k - \bar{B_n})^2.
\]

This is not an i.i.d. sample and not consistent in general.
I am not sure what the finite sample properties are here.

<br/>
<br/>

Overlapping batch means overlap and can be consistent. Since I am not concerned with consistency here, I will skip this one. 


<h2>Quantitive Measurements</h2>

Generally, we want to approximate $\pi f = \int f d\pi$ by 

\[
\pi_n f = \frac{1}{n} \sum_{k = 1}^n f(X_k)
\]

for some class of functions $f \in \mathcal{F}$.
Chebyshev and convexity gives us concentration for $f \in L_2(\pi)$ by

\[
P_x( \norm{\pi_n f - \pi f}_2^2 \ge \epsilon)
\le \epsilon^{-2} E_x \norm{\pi_n f - \pi f}_2^2
\le \epsilon^{-2} \frac{1}{n} \sum_{k = 1}^n E_x \norm{f(X_k) - \pi f}_2^2.
\]

We can try to control any of these or estimate them.

Batch means estimators try to estimate the variance here.

<br>
<br>

Total variation controls bounded and measurable functions.
Particularly, indicator functions to approximate probabilities

\[
P_x(X \in B) \approx \frac{1}{n} \sum_{k = 1}^n I(X_k \in B).
\]

Let $f : \mathbb{R}^d \to \mathbb{R}$ with $|f| \le R$. Then

\[
E_x| \pi_n f - \pi f |
\le \frac{1}{n} \sum_{k = 1}^n E_x|f(X_k) - \pi f|
\le \frac{1}{n} \sum_{k = 1}^n 2R \int_{\mathbb{R}^d}|\delta_x P^k - \pi| dm
= \le \frac{2R}{n} \sum_{k = 1}^n \norm{\delta_x P^k - \pi}_{TV}.
\] 

<br>
<br>

Wasserstein 1 and Wasserstein 2 control Lipschitz functions.
Let $f : \mathbb{R}^d \to \mathbb{R}$ be $M-$Lipschitz.
Let $\Gamma_k$ be an optimal coupling for $\delta_x P^k$ and $\pi$.
Then 
\[
| E_x \pi_n f - \pi f |
\le \frac{M}{n} \sum_{k = 1}^n | E_x X_k - EX_0 |
\le \frac{M}{n} \sum_{k = 1}^n \int_{\mathbb{R}^{2d}} |x - y| d\Gamma(x, y)
= \frac{M}{n} \sum_{k = 1}^n W_1(\delta_x P^k, \pi).
\]




<h2>Geometric Convergence of Langevin Monte Carlo</h2>

We want to sample from a probability distribution on $\pi$ on $\mathbb{R}^d$ such that

$\pi dm = \frac{1}{Z} exp(-U) dm$

where m is Lebesgue measure.
The approach is to approximate a continuous diffusion process with invariant distribution $\pi$.

This is done by solving a differential equation in the following way.
Let $(P_t)_{t \ge 0}$ be a diffusion Markov Process with generator $L$ that is self-adjoint in $L_2(\pi)$ and solves

\[
\partial_t P_t f = L P_t f
\]

with initial point $f \in L_2(\pi)$.
Thus,

\[
P_t f = f + \int_0^t L P_s f ds
\]

This says that generator $L$ is like a derivative of some objective and $P_t f$ is optimizing this objective.
The spectral theorem provides that $P_tf \to \pi f$ in $L_2(\pi)$.


<h2>Basic Properties of Metropolis-Adjusted Langevin Monte Carlo</h2>

Adding a Metropolis-Hastings accept/reject step using the Markov kernel $P$ as a proposal will allow $\pi$ to be invariant.
This is called the Metropolis Adjusted Langevin Algorithm (MALA).
A Markov Kernel that is self-adjoint in $L_2(\pi)$ is 

\[
M(x, B) = \int_{y \in B} p(x, y) \alpha(x, y) dy + \delta_x(B)(1 - \int_{y \in \mathbb{R}^d} p(x, y) \alpha(x, y) dy)
\]

with $\alpha(x, y) = 1 \wedge \frac{\pi(y) p(y, x)}{\pi(x) p(x, y)}$.

The Markov chain $(X_k)$ with this kernel can be simulated on a computer in the following way.
Propose the next step $Y$ from the proposal $p(X_k, \cdot)$ and with probability $\alpha(X_k, Y) = 1 \wedge \frac{\pi(Y) p(Y, X_k)}{\pi(X_k) p(X_k, Y)}$, accept it and set $X_{k + 1} = Y$ otherwise reject it and set $X_{k + 1} = X_k$.


<br>
<br>

Because $p$ is strictly positive, the chain is irreducible with respect to Lebesgue measure. Also because of $p$ is aperiodic, it is also aperiodic.

This is also irreducible with respect to $\pi dm$ because $\pi$ is strictly positive and so the chain is Harris recurrent.

Thus, we get total variation convergence and the mean ergodic theorem.
For any $f \in L_1(\pi)$, we have 
\[
\frac{1}{n} \sum_{k = 1}^n f(X_k) \to \int_{\mathbb{R}^d} f \pi dm
\]
a.e. $\pi$ and for any initial distribution $\nu$, we have

\[
\norm{\nu P^n - \pi}_{TV} \to 0.
\]

However, we do not know the rate or the constants. 
The constants could be $10^{100}$ and the rate could be $\log\log\log(n)$.
This result actually tells us nothing useful.
To establish a geometric rate, we need to establish the drift condition.

<br>
<br>

TODO
Is it Feller?

Are compact sets small?



<h2>Basic Properties of Euler Langevin Monte Carlo</h2>

We discretize the continuous time Langevin diffusion process

\[
X_t = x -\int_0^t \nabla U(X_t) dt + \sqrt{2} W_t
\]

over the interval [0, T] where $x$ is a deterministic initial value and $W_t$ is Brownian motion. 
The Euler discretization comes from ordinary differential equations and partitions $[0, T] = [0, hn, \ldots, h(n - 1), hn]$

with a non-anticipating approximation

\[
X_t = x - \sum_{k = 0}^{n - 1} \int_{kh}^t \nabla U(X_{kh}) I(t \in [kh, (k + 1)h) dt + \sqrt{2} W_t
\]

Because $W_{(k + 1)h}  - W_{k}$ is $N(0, h)$, we have

\[
X_{(k + 1)h} = X_k - h \nabla U(X_{kh}) + \sqrt{2h} \xi
\]

where $\xi$ is $N(0, 1)$.
The Markov chain is then $(X_k)_{k = 0}^n = (X_{kh})_{k = 0}^n$ with initial point $X_0 = x$.
The Markov kernel has a normal density with respect to Lebesgue measure on $(\mathbb{R}^d, \mathcal{B}(\mathbb{R}^d))$ and is

\[
P(x, B) = \int_{y \in B} \left(\frac{1}{4 \pi h}\right)^{\frac{d}{2}} \exp{\left(\frac{1}{4h} \norm{y - x + h \nabla U(x)}_2^2\right)} dy
\]

for any measurable set $B$.

The Markov kernel $P$ is irreducible because $P(x, \mathbb{R}^d) = 1$.

If $\nabla U$ is continuous, $x \mapsto K(\cdot, x)$ is continuous in weak* topology and strictly positive.

<br/>
<br/>

<b>Lemma. </b><i>P is aperiodic and irreducible with respect to Lebesgue measure.</i><br/><br/>
<i>Proof.</i>
$P(x, \mathbb{R}^d) = 1$ and P(x, y) > 0.

<br/>
<br/>

<b>Lemma. </b><i>If $\nabla U$ is continuous, P is Feller and we have a minorization by uniform Lebesgue measure on all compact sets.</i><br/><br/>
<i>Proof.</i>
The map $x \mapsto P(x, \cdot)$ is continuous in the weak* topology by dominated convergence and it is Feller.

Let $C \times D$ be a product of two compact sets.
This is compact.
P is jointly continuous and strictly positive.
Thus, for some $\alpha$, we have
\[
\min_{x, y \in C \times D} P(x, y) \ge \alpha
\]
and
\[
\min_{x \in C} P(x, y) \ge \alpha \frac{m(y)}{m(D)} I(y \in D)
\]
for all $y \in \mathbb{R}^d$ where $m$ is Lebesgue measure.

Unlike the continuous time diffusion, we do not know if there is an invariant distribution.
If there exists a $V : \mathbb{R}^d \to \mathbb{R}_+$ with compact sublevel sets and

\[
LV = PV - V \le -aV + b
\]

with some $a, b \ge 0$, then there exists an invariant distribution $\pi_h$ for the discretized diffusion and the convergence in total variation is geometrically fast.

TODO expand on this.

There are some other conditions in the MALA paper.

<h2>References.</h2>
TODO

</body>
</html>