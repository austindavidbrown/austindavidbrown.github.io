---
title: "Batch Means for Variance Estimation in Langevin Monte Carlo"
date: "2019-05-27"
categories:
- Langevin Monte Carlo
- Monte Carlo
- MCMC
tags:
- Langevin Monte Carlo
- Monte Carlo
- MCMC
---
$
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
$

This approach is to estimate the variance using the Markov chain.

<h3>Batch Means</h3>

Let $(X_k)$ be the Markov chain and $f : \mathbb{R}^d \to \mathbb{R}$.
The functional CLT is the motivation here.
Divide the chain $(X_k)$ into $m$ non-overlapping batches of length $b$ so that $n = b m$.
Determining the optimal batch size is an open problem.
Estimate the each batch mean by $B_k = \frac{1}{b} \sum_{i = 1}^{b} f(X_{bk + i})$.
The estimate of the variance of the batch mean is

\[
S^2_{BM} = \frac{b}{m} \sum_{k = 0}^{m - 1} (B_k - \bar{B_m})^2.
\]

Does this actually converge and to what? This is not an i.i.d. sample and not consistent in general.
I am not sure what the finite sample properties are here.

<h3>Batch Means for OU Process</h3>

The Ornstein-Uhlenbeck Markov semigroup $(P_t)$ on $\mathbb{R}^d$ with the standard normal distribution as its invariant measure can be derived from the SDE 

\[
dX_t = -X_t dt + \sqrt{2} dW_t
\]

starting at $X_0$.
This semigroup has an explicit solution $X_t$ being $N(e^{-t}X_0, 1 - e^{-2t})$ which we could simulate directly.
Instead, we will simulate this with the Metropolis Adjusted Langevin Monte Carlo Algorithm (MALA) using Pytorch and look at the batch means estimate.
We simulate this in $\mathbb{R}^2$ using Euler discretization and purposefully not optimal discretization steps $h = .1$ and initial point $X_0 = (3, 3)$.
The length of the chain is $10^4$ with $10^2$ batch means.

<pre class="prettyprint lang-python">
import torch
from torch.distributions import Normal

torch.no_grad() 
torch.manual_seed(0)
torch.set_default_dtype(torch.float64)

def OULMC(N, h, X_0):
  X = torch.zeros(N, 2)
  X[0] = X_0

  for i in range(0, N - 1):
    Z = torch.normal(torch.zeros(2), 1)
    Y = X[i] - h * X[i] + torch.sqrt(torch.tensor(2 * h)) * Z

    u = torch.zeros(1).uniform_(0, 1).item()

    R = (1/2.0 * (torch.norm(X[i], p = 2)**2 - torch.norm(Y, p = 2)**2)) + 1/(4.0 * h) * (torch.norm(Y - 1 * X[i] + h * X[i], p = 2)**2 - torch.norm(X[i] - Y + h * Y, p = 2)**2)
    a = min(1, torch.exp(R).item())
    if u <= a:
      X[i + 1] = Y
    else:
      X[i + 1] = X[i]
  return X

def bmSE(X, M):
  N = X.size(0)
  M = M # how many batches
  B = int(N/M) # batch size

  S2s = []
  mu = torch.mean(X.narrow(0, 0, B * M), 0)
  for m in range(0, M):
    B_m = torch.mean(X.narrow(0, m * B, B), 0)
    S2_m = torch.pow(B_m - mu, 2)
    S2s.append(S2_m)
  se = torch.sqrt(B * torch.mean(torch.stack(S2s), 0))
  return se

N = 10**4
h = .1
X_0 = torch.zeros(2).fill_(3)
X = OULMC(N, h, X_0)
se = bmSE(X, 100)

print(torch.mean(X, 0))
print(se)
</pre>

<pre>
</pre>

<h3>Overlapping Batch Means</h3>

Overlapping batch means overlap and can be consistent. Since I am not concerned with consistency here, I will skip this one. 


<h2>References.</h2>

Boucheron, St√©phane; Thomas, Maud. Concentration inequalities for order statistics. Electron. Commun. Probab. 17 (2012), paper no. 51, 12 pp. doi:10.1214/ECP.v17-2210. https://projecteuclid.org/euclid.ecp/1465263184

<!--  Twitter -->
<br/>
<br/>